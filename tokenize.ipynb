{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/askreddit.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format\n",
    "-----------\n",
    "\n",
    "`{author_fullname, score, fullname, parent_id, body, is_submitter}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'author_fullname': 't2_5m13p', 'body': \"The 1963 version of Shirley Jackson's creepy book The Haunting. No special effects, no CGI, no rubber monster, just good, scary writing and acting. All alone. In the night. In the dark...\\n\", 'fullname': 't1_e75tb7l', 'is_submitter': False, 'parent_id': 't3_9lcjo3', 'score': 3443}\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's tokenize the body part\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "def dataset_preparation(data):\n",
    "    # split text to lines\n",
    "    corpus = data.lower().split(\"\\n\")\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        # List of words (I hope)\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            # Let's do predictions based on all of the words before\n",
    "            # the actual word.\n",
    "            # see n_gram: https://medium.com/@shivambansal36/language-modelling-text-generation-using-lstms-deep-learning-for-nlp-ed36b224b275\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "            max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences,   \n",
    "                          maxlen=max_sequence_len, padding='pre'))\n",
    "    return input_sequences, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 31\n"
     ]
    }
   ],
   "source": [
    "input_seq, total_words = dataset_preparation(data[0][\"body\"])\n",
    "predictors, label = input_seq[:,:-1], input_seq[:,-1]\n",
    "print( len(predictors), len(label))\n",
    "\n",
    "# The following example dataframe contains the tokenized value of a word. (first column)\n",
    "# all the other columns represent words which occurred before the word in the same line.\n",
    "df = pd.DataFrame(data=np.column_stack((label, predictors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1   2   3   4   5   6   7   8   9  ...  22  23  24  25  26  27  28  \\\n",
      "0    4   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   0   \n",
      "1    5   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   0   \n",
      "2    6   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   0   \n",
      "3    7   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   1   \n",
      "4    8   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   1   4   \n",
      "5    9   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   1   4   5   \n",
      "6   10   0   0   0   0   0   0   0   0   0 ...   0   0   0   1   4   5   6   \n",
      "7    1   0   0   0   0   0   0   0   0   0 ...   0   0   1   4   5   6   7   \n",
      "8   11   0   0   0   0   0   0   0   0   0 ...   0   1   4   5   6   7   8   \n",
      "9    2   0   0   0   0   0   0   0   0   0 ...   1   4   5   6   7   8   9   \n",
      "10  12   0   0   0   0   0   0   0   0   0 ...   4   5   6   7   8   9  10   \n",
      "11  13   0   0   0   0   0   0   0   0   0 ...   5   6   7   8   9  10   1   \n",
      "12   2   0   0   0   0   0   0   0   0   0 ...   6   7   8   9  10   1  11   \n",
      "13  14   0   0   0   0   0   0   0   0   0 ...   7   8   9  10   1  11   2   \n",
      "14   2   0   0   0   0   0   0   0   0   0 ...   8   9  10   1  11   2  12   \n",
      "15  15   0   0   0   0   0   0   0   0   0 ...   9  10   1  11   2  12  13   \n",
      "16  16   0   0   0   0   0   0   0   0   0 ...  10   1  11   2  12  13   2   \n",
      "17  17   0   0   0   0   0   0   0   0   0 ...   1  11   2  12  13   2  14   \n",
      "18  18   0   0   0   0   0   0   0   0   0 ...  11   2  12  13   2  14   2   \n",
      "19  19   0   0   0   0   0   0   0   0   0 ...   2  12  13   2  14   2  15   \n",
      "20  20   0   0   0   0   0   0   0   0   0 ...  12  13   2  14   2  15  16   \n",
      "21  21   0   0   0   0   0   0   0   0   0 ...  13   2  14   2  15  16  17   \n",
      "22  22   0   0   0   0   0   0   0   0   1 ...   2  14   2  15  16  17  18   \n",
      "23  23   0   0   0   0   0   0   0   1   4 ...  14   2  15  16  17  18  19   \n",
      "24  24   0   0   0   0   0   0   1   4   5 ...   2  15  16  17  18  19  20   \n",
      "25   3   0   0   0   0   0   1   4   5   6 ...  15  16  17  18  19  20  21   \n",
      "26   1   0   0   0   0   1   4   5   6   7 ...  16  17  18  19  20  21  22   \n",
      "27  25   0   0   0   1   4   5   6   7   8 ...  17  18  19  20  21  22  23   \n",
      "28   3   0   0   1   4   5   6   7   8   9 ...  18  19  20  21  22  23  24   \n",
      "29   1   0   1   4   5   6   7   8   9  10 ...  19  20  21  22  23  24   3   \n",
      "30  26   1   4   5   6   7   8   9  10   1 ...  20  21  22  23  24   3   1   \n",
      "\n",
      "    29  30  31  \n",
      "0    0   0   1  \n",
      "1    0   1   4  \n",
      "2    1   4   5  \n",
      "3    4   5   6  \n",
      "4    5   6   7  \n",
      "5    6   7   8  \n",
      "6    7   8   9  \n",
      "7    8   9  10  \n",
      "8    9  10   1  \n",
      "9   10   1  11  \n",
      "10   1  11   2  \n",
      "11  11   2  12  \n",
      "12   2  12  13  \n",
      "13  12  13   2  \n",
      "14  13   2  14  \n",
      "15   2  14   2  \n",
      "16  14   2  15  \n",
      "17   2  15  16  \n",
      "18  15  16  17  \n",
      "19  16  17  18  \n",
      "20  17  18  19  \n",
      "21  18  19  20  \n",
      "22  19  20  21  \n",
      "23  20  21  22  \n",
      "24  21  22  23  \n",
      "25  22  23  24  \n",
      "26  23  24   3  \n",
      "27  24   3   1  \n",
      "28   3   1  25  \n",
      "29   1  25   3  \n",
      "30  25   3   1  \n",
      "\n",
      "[31 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label should be one-hot encoded for learning\n",
    "import keras.utils as k_utils\n",
    "label = k_utils.to_categorical(label, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
